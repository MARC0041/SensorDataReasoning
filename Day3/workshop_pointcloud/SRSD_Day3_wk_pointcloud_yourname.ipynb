{"cells":[{"cell_type":"markdown","source":["# Workshop on design and build a spatial reasoning system: Point cloud classification using PointNet\n","\n","Course: Spatial Reasoning from Sensor Data, https://www.iss.nus.edu.sg/executive-education/course/detail/spatial-reasoning-from-sensor-data/artificial-intelligence\n","\n","Contact: Tian Jing\n","\n","Email: tianjing@nus.edu.sg\n","\n","## Objective\n","\n","- Visualize the point cloud data\n","- Perform point cloud classification using the PointNet approach\n","\n","Reference: https://datascienceub.medium.com/pointnet-implementation-explained-visually-c7e300139698"],"metadata":{"id":"nV2FHVfbs3rM"},"id":"nV2FHVfbs3rM"},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision.datasets import MNIST\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader, random_split\n","from tqdm.notebook import tqdm\n"],"metadata":{"id":"EZcDNuMY_ryd"},"id":"EZcDNuMY_ryd","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"fdc85946","metadata":{"id":"fdc85946"},"outputs":[],"source":["# Prepare a MNIST3D dataset\n","\n","def transform_img2pc(img):\n","    img_array = np.asarray(img)\n","    indices = np.argwhere(img_array > 127)\n","    return indices.astype(np.float32)\n","\n","dataset = MNIST(root='./data', train=True, download=True)\n","len_points = []\n","for idx in range(len(dataset)):\n","    img,label = dataset[idx]\n","    pc = transform_img2pc(img)\n","    len_points.append(len(pc))\n","\n","\n","class MNIST3D(Dataset):\n","    \"\"\"3D MNIST dataset.\"\"\"\n","\n","    NUM_CLASSIFICATION_CLASSES = 10\n","    POINT_DIMENSION = 3\n","\n","    def __init__(self, dataset, num_points=500):\n","        self.dataset = dataset\n","        self.number_of_points = num_points\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","\n","        img,label = dataset[idx]\n","        pc = transform_img2pc(img)\n","\n","        if self.number_of_points-pc.shape[0]>0:\n","            # Duplicate points\n","            sampling_indices = np.random.choice(pc.shape[0], self.number_of_points-pc.shape[0])\n","            new_points = pc[sampling_indices, :]\n","            pc = np.concatenate((pc, new_points),axis=0)\n","        else:\n","            sampling_indices = np.random.choice(pc.shape[0], self.number_of_points)\n","            pc = pc[sampling_indices, :]\n","\n","        pc = pc.astype(np.float32)\n","        # add z\n","        noise = np.random.normal(0,0.05,len(pc))\n","        noise = np.expand_dims(noise, 1)\n","        pc = np.hstack([pc, noise]).astype(np.float32)\n","        pc = torch.tensor(pc)\n","\n","        return pc, label\n"]},{"cell_type":"code","execution_count":null,"id":"cf7a40eb","metadata":{"id":"cf7a40eb"},"outputs":[],"source":["dataset = 'MNIST3D'\n","number_of_points = 200\n","batch_size = 128\n","learning_rate = 0.001\n"]},{"cell_type":"code","execution_count":null,"id":"5e7ada95","metadata":{"id":"5e7ada95"},"outputs":[],"source":["\n","train_dataset = MNIST(root='./data/MNIST', download=True, train=True)\n","test_dataset = MNIST(root='./data/MNIST', download=True, train=False)\n","dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n","\n","dataset_3d = MNIST3D(dataset, number_of_points)\n","l_data = len(dataset_3d)\n","train_dataset, val_dataset, test_dataset = random_split(dataset_3d,\n","                                          [round(0.8*l_data), round(0.1*l_data), round(0.1*l_data)],\n","                                          generator=torch.Generator().manual_seed(1))\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"id":"fee56990","metadata":{"id":"fee56990"},"outputs":[],"source":["# Visualize a point cloud\n","\n","pc = train_dataset[1][0].numpy()\n","label = train_dataset[1][1]\n","fig = plt.figure(figsize=[7,7])\n","ax = plt.axes(projection='3d')\n","sc = ax.scatter(pc[:,0], pc[:,1], pc[:,2], c=pc[:,0] ,s=80, marker='o', cmap=\"viridis\", alpha=0.7)\n","ax.set_zlim3d(-1, 1)\n","plt.title(f'Label: {label}')\n"]},{"cell_type":"code","source":["# Define the PointNet model\n","\n","class TransformationNet(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim):\n","        super(TransformationNet, self).__init__()\n","        self.output_dim = output_dim\n","\n","        self.conv_1 = nn.Conv1d(input_dim, 64, 1)\n","        self.conv_2 = nn.Conv1d(64, 128, 1)\n","        self.conv_3 = nn.Conv1d(128, 256, 1)\n","\n","        self.bn_1 = nn.BatchNorm1d(64)\n","        self.bn_2 = nn.BatchNorm1d(128)\n","        self.bn_3 = nn.BatchNorm1d(256)\n","        self.bn_4 = nn.BatchNorm1d(256)\n","        self.bn_5 = nn.BatchNorm1d(128)\n","\n","        self.fc_1 = nn.Linear(256, 256)\n","        self.fc_2 = nn.Linear(256, 128)\n","        self.fc_3 = nn.Linear(128, self.output_dim*self.output_dim)\n","\n","    def forward(self, x):\n","        num_points = x.shape[1]\n","        x = x.transpose(2, 1)\n","        x = F.relu(self.bn_1(self.conv_1(x)))\n","        x = F.relu(self.bn_2(self.conv_2(x)))\n","        x = F.relu(self.bn_3(self.conv_3(x)))\n","\n","        x = nn.MaxPool1d(num_points)(x)\n","        x = x.view(-1, 256)\n","\n","        x = F.relu(self.bn_4(self.fc_1(x)))\n","        x = F.relu(self.bn_5(self.fc_2(x)))\n","        x = self.fc_3(x)\n","\n","        identity_matrix = torch.eye(self.output_dim)\n","        if torch.cuda.is_available():\n","            identity_matrix = identity_matrix.cuda()\n","        x = x.view(-1, self.output_dim, self.output_dim) + identity_matrix\n","        return x\n","\n","\n","class BasePointNet(nn.Module):\n","\n","    def __init__(self, point_dimension):\n","        super(BasePointNet, self).__init__()\n","        self.input_transform = TransformationNet(input_dim=point_dimension, output_dim=point_dimension)\n","        self.feature_transform = TransformationNet(input_dim=64, output_dim=64)\n","\n","        self.conv_1 = nn.Conv1d(point_dimension, 64, 1)\n","        self.conv_2 = nn.Conv1d(64, 64, 1)\n","        self.conv_3 = nn.Conv1d(64, 64, 1)\n","        self.conv_4 = nn.Conv1d(64, 128, 1)\n","        self.conv_5 = nn.Conv1d(128, 256, 1)\n","\n","        self.bn_1 = nn.BatchNorm1d(64)\n","        self.bn_2 = nn.BatchNorm1d(64)\n","        self.bn_3 = nn.BatchNorm1d(64)\n","        self.bn_4 = nn.BatchNorm1d(128)\n","        self.bn_5 = nn.BatchNorm1d(256)\n","\n","\n","    def forward(self, x, plot=False):\n","        num_points = x.shape[1]\n","\n","        input_transform = self.input_transform(x) # T-Net tensor [batch, 3, 3]\n","        x = torch.bmm(x, input_transform) # Batch matrix-matrix product\n","        x = x.transpose(2, 1)\n","        tnet_out=x.cpu().detach().numpy()\n","\n","        x = F.relu(self.bn_1(self.conv_1(x)))\n","        x = F.relu(self.bn_2(self.conv_2(x)))\n","        x = x.transpose(2, 1)\n","\n","        feature_transform = self.feature_transform(x) # T-Net tensor [batch, 64, 64]\n","        x = torch.bmm(x, feature_transform)\n","        x = x.transpose(2, 1)\n","        x = F.relu(self.bn_3(self.conv_3(x)))\n","        x = F.relu(self.bn_4(self.conv_4(x)))\n","        x = F.relu(self.bn_5(self.conv_5(x)))\n","        x, ix = nn.MaxPool1d(num_points, return_indices=True)(x)  # max-pooling\n","        x = x.view(-1, 256)  # global feature vector\n","\n","        return x, feature_transform, tnet_out, ix\n","\n","\n","class ClassificationPointNet(nn.Module):\n","\n","    def __init__(self, num_classes, dropout=0.3, point_dimension=3):\n","        super(ClassificationPointNet, self).__init__()\n","        self.base_pointnet = BasePointNet(point_dimension=point_dimension)\n","\n","        self.fc_1 = nn.Linear(256, 128)\n","        self.fc_2 = nn.Linear(128, 64)\n","        self.fc_3 = nn.Linear(64, num_classes)\n","\n","        self.bn_1 = nn.BatchNorm1d(128)\n","        self.bn_2 = nn.BatchNorm1d(64)\n","\n","        self.dropout_1 = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x, feature_transform, tnet_out, ix_maxpool = self.base_pointnet(x)\n","\n","        x = F.relu(self.bn_1(self.fc_1(x)))\n","        x = F.relu(self.bn_2(self.fc_2(x)))\n","        x = self.dropout_1(x)\n","\n","        return F.log_softmax(self.fc_3(x), dim=1), feature_transform, tnet_out, ix_maxpool\n"],"metadata":{"id":"ycw_6xYaHiyf"},"id":"ycw_6xYaHiyf","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"a5735853","metadata":{"id":"a5735853"},"outputs":[],"source":["\n","model = ClassificationPointNet(num_classes=dataset_3d.NUM_CLASSIFICATION_CLASSES,point_dimension=dataset_3d.POINT_DIMENSION)\n","if torch.cuda.is_available():\n","    model.cuda()\n","    device = 'cuda'\n","else:\n","    device = 'cpu'\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"]},{"cell_type":"code","execution_count":null,"id":"bb0cdfe5","metadata":{"id":"bb0cdfe5"},"outputs":[],"source":["# Train the model\n","epochs = 10\n","train_loss = []\n","test_loss = []\n","train_acc = []\n","test_acc = []\n","best_loss= np.inf\n","\n","for epoch in tqdm(range(epochs)):\n","    epoch_train_loss = []\n","    epoch_train_acc = []\n","\n","    # training loop\n","    for data in train_dataloader:\n","        points, targets = data  # [batch, num_points, 3] [batch]\n","\n","        if torch.cuda.is_available():\n","            points, targets = points.cuda(), targets.cuda()\n","        if points.shape[0] <= 1:\n","            continue\n","        optimizer.zero_grad()\n","        model = model.train()\n","        preds, feature_transform, tnet_out, ix_maxpool = model(points)\n","\n","        identity = torch.eye(feature_transform.shape[-1])\n","        if torch.cuda.is_available():\n","            identity = identity.cuda()\n","        regularization_loss = torch.norm(\n","            identity - torch.bmm(feature_transform, feature_transform.transpose(2, 1)))\n","        # Loss\n","        loss = F.nll_loss(preds, targets) + 0.001 * regularization_loss\n","        epoch_train_loss.append(loss.cpu().item())\n","        loss.backward()\n","        optimizer.step()\n","        preds = preds.data.max(1)[1]\n","        corrects = preds.eq(targets.data).cpu().sum()\n","\n","        accuracy = corrects.item() / float(batch_size)\n","        epoch_train_acc.append(accuracy)\n","\n","    epoch_test_loss = []\n","    epoch_test_acc = []\n","\n","    # validation loop\n","    for batch_number, data in enumerate(test_dataloader):\n","        points, targets = data\n","        if torch.cuda.is_available():\n","            points, targets = points.cuda(), targets.cuda()\n","        model = model.eval()\n","        preds, feature_transform, tnet_out, ix = model(points)\n","        loss = F.nll_loss(preds, targets)\n","        epoch_test_loss.append(loss.cpu().item())\n","        preds = preds.data.max(1)[1]\n","        corrects = preds.eq(targets.data).cpu().sum()\n","        accuracy = corrects.item() / float(batch_size)\n","        epoch_test_acc.append(accuracy)\n","\n","    print('Epoch %s: train loss: %s, val loss: %f, train accuracy: %s,  val accuracy: %f'\n","              % (epoch,\n","                round(np.mean(epoch_train_loss), 4),\n","                round(np.mean(epoch_test_loss), 4),\n","                round(np.mean(epoch_train_acc), 4),\n","                round(np.mean(epoch_test_acc), 4)))\n","\n","    if np.mean(test_loss) < best_loss:\n","        state = {\n","            'model':model.state_dict(),\n","            'optimizer':optimizer.state_dict()\n","        }\n","        best_loss=np.mean(test_loss)\n","\n","    train_loss.append(np.mean(epoch_train_loss))\n","    test_loss.append(np.mean(epoch_test_loss))\n","    train_acc.append(np.mean(epoch_train_acc))\n","    test_acc.append(np.mean(epoch_test_acc))\n"]},{"cell_type":"code","execution_count":null,"id":"7340ae1a","metadata":{"id":"7340ae1a"},"outputs":[],"source":["# Define inference function\n","def infer(dataset,\n","          model,\n","          point_cloud_file,\n","          shuffle_points=False,\n","          plot_tNet_out=True,\n","          return_indices_maxpool=False):\n","\n","    num_classes = dataset.NUM_CLASSIFICATION_CLASSES\n","    points, label = point_cloud_file\n","\n","    if torch.cuda.is_available():\n","        points = points.cuda()\n","        model.cuda()\n","\n","    points = points.unsqueeze(dim=0)\n","    model = model.eval()\n","    preds, feature_transform, tnet_out, ix = model(points)\n","    preds = preds.data.max(1)[1]\n","\n","    points = points.cpu().numpy().squeeze()\n","    preds = preds.cpu().numpy()\n","\n","    if return_indices_maxpool:\n","        return preds, tnet_out, ix\n","\n","    return preds, tnet_out"]},{"cell_type":"code","source":["# Test data\n","\n","SAMPLE = 0\n","\n","fig = plt.figure(figsize=[12,6])\n","ax = fig.add_subplot(1, 2, 1, projection='3d')\n","\n","# plot input sample\n","pc = test_dataset[SAMPLE][0].numpy()\n","label = test_dataset[SAMPLE][1]\n","sc = ax.scatter(pc[:,0], pc[:,1], pc[:,2], c=pc[:,0] ,s=50, marker='o', cmap=\"viridis\", alpha=0.7)\n","preds, tnet_out = infer(dataset_3d, model,test_dataset[SAMPLE])\n","ax.set_xlabel('x')\n","ax.set_ylabel('y')\n","ax.set_zlim3d(-1, 1)\n","ax.title.set_text(f'Input point cloud - Predict: {preds}')\n","\n","ax = fig.add_subplot(1, 2, 2, projection='3d')\n","SAMPLE = SAMPLE + 1\n","pc = test_dataset[SAMPLE][0].numpy()\n","label = test_dataset[SAMPLE][1]\n","sc = ax.scatter(pc[:,0], pc[:,1], pc[:,2], c=pc[:,0] ,s=50, marker='o', cmap=\"viridis\", alpha=0.7)\n","preds, tnet_out = infer(dataset_3d, model,test_dataset[SAMPLE])\n","ax.set_xlabel('x')\n","ax.set_ylabel('y')\n","ax.set_zlim3d(-1, 1)\n","ax.title.set_text(f'Input point cloud - Predict: {preds}')\n"],"metadata":{"id":"krIlOncruwFd"},"id":"krIlOncruwFd","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9f59d9fc","metadata":{"id":"9f59d9fc"},"outputs":[],"source":["# Evaluate accuracy on test set\n","corrects=0\n","\n","for i in tqdm(range(len(test_dataset))):\n","    target=test_dataset[i][1]\n","    pred, tnet_out = infer(dataset_3d, model, test_dataset[i], shuffle_points=False, plot_tNet_out=False)\n","    if target == pred:\n","        corrects+=1\n","\n","print(f'Accuracy = {corrects/len(test_dataset)}')"]},{"cell_type":"markdown","source":["\n","$\\color{red}{\\text{Discussions}}$\n","\n","Q1. State one reason that the traditional convolution cannot be directly applied to the point cloud data.\n","\n","Q2. State one unique function of the PointNet that can aggregate information from unordered data (e.g., the input point cloud data is permuted).\n"],"metadata":{"id":"DMJd7IJ0DNwq"},"id":"DMJd7IJ0DNwq"},{"cell_type":"code","source":["# Provide your answers to Q1 here\n","#\n","#\n","#\n","# Provide your answers to Q2 here\n","#\n","#"],"metadata":{"id":"J6DDjkI4DWa3"},"id":"J6DDjkI4DWa3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**After you finish the workshop, rename and submit your .ipynb file.**"],"metadata":{"id":"KyKlFmi3DT_7"},"id":"KyKlFmi3DT_7"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"12RQDCV7krZtfjwJ0B4bOEBnvnDHTu-k2","timestamp":1703335154334}]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}